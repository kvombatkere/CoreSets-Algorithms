\documentclass[12pt]{amsart}
\RequirePackage[l2tabu, orthodox]{nag}
\usepackage[main=english]{babel}
\usepackage[rm={lining,tabular},sf={lining,tabular},tt={lining,tabular,monowidth}]{cfr-lm}
\usepackage{amsthm,amssymb,latexsym,gensymb,mathtools,mathrsfs,bbm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf,enumitem,microtype,dcolumn,booktabs,hyperref,url,fancyhdr}
\usepackage[margin=2.7182818284590452353602874713526624977572470936999cm]{geometry}
\pagestyle{fancy}
\setlength{\headheight}{15pt}

% Plotting
\usepackage{pgfplots}
\usepackage{xinttools} % for the \xintFor***
\usepgfplotslibrary{fillbetween}
\pgfplotsset{compat=newest}
\usepackage{tikz}

% For embedding images
\graphicspath{ {./images/} }

% Custom Commands
\newcommand*{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*{\abs}[1]{\left\lvert#1\right\rvert}
\newcommand*{\suchthat}{\,\mathrel{\big|}\,}

\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Prob}{\mathbb{P}}
\DeclarePairedDelimiterX\innerp[2]{(}{)}{#1\delimsize\vert\mathopen{}#2}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\esssup}{ess \ sup}
\newcommand*{\dual}[2]{\left\langle#1,#2\right\rangle}
\DeclareMathOperator{\sgn}{sgn}

% Formatting definitions, propositions, etc.
\theoremstyle{plain} 
\newtheorem{definition}{Definition}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem*{thm}{Theorem}
\newtheorem{corollary}{Corollary}

\setlist{topsep=1ex,parsep=1ex,itemsep=0ex}
\setlist[1]{leftmargin=\parindent}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}
\setlist[enumerate,2]{label=(\alph*),ref=(\alph*)}
\begin{document}

\begin{center}
Andrew Roberts \\
Algorithmic Techniques for Taming Big Data \\
Bayesian Coresets Notes for Final Project
\end{center}
\smallskip

\section{Plan/To-Do List}
\subsection{Implement the tests in the paper on posterior inference for unknown mean $N(\mu, I)$}

\section{Estimation of MVN Mean with Known Covariance}
For data $y = (y_1, \dots, y_N)$ with $y_i \in \mathbb{R}^2$ we assume the following model: 
\[(y_1|\theta), \dots, (y_N|\theta) \text{ are iid with } y_i|\theta \sim N_2(\theta, I_2)\]
The covariance matrix $I_2$ is assumed to be known. We also assume a prior distribution on the mean vector: 
\[\theta \sim N_2(\mu_0, I_2)\]
It is known that this likelihood, prior combination form a conjugate pair with posterior: 
\[\theta|y \sim N_2(\mu, \Sigma)\]
where 
\begin{align*} 
&\Sigma = \frac{1}{1 + N}I && \mu = \Sigma\left(\mu_0 + \sum_{i = 1}^{N} y_i\right) = \frac{\mu_0}{1 + N} + \frac{S_N}{1 + N}
\end{align*} 

I emphasize that inference is only being done on the mean vector, with the underlying covariance matrix $I_2$ fixed. $\Sigma$ is the covariance
matrix of the posterior distribution for the mean vector. Since this posterior is known in closed form, we can easily test the quality of the coreset posterior approximation
against the true posterior. 


\section{Bayesian Regression}
Resource: https://statswithr.github.io/book/introduction-to-bayesian-regression.html

\subsection{Setup}
\subsection{Likelihood}
\subsection{Priors}

\section{Hilbert Coresets Code}
Code from paper: https://github.com/trevorcampbell/bayesian-coresets

\subsection{Required Inputs}
\begin{itemize} 
\item The data point log-likelihoods: $\mathcal{L}_i := \log L(\theta; y_i) = \log p(y_i|\theta)$
\item $M = $ coreset size (set this based on theoretical guarantees)
\item Weight function $\hat{\pi}$
\item Random projection parameter $J$
\end{itemize} 

\subsection{Functions/Sub-Routines}
\begin{itemize} 
\item Function to simulate dataset $(y, X)$ (question: how to simulate $X$? Reasonable to add some feature correlation) 
\item 
\end{itemize} 



\end{document} 

